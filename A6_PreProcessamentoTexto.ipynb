{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-castle",
   "metadata": {},
   "source": [
    "Basicamente o que a questão pede é para processar cada um dos documentos e depois calcular o numero de palavras repetidas. Perceba que o relevante da questão é justamente a primeira parte, essa contagem de repetidas é só para sabermos se tudo foi feito corretamente. \n",
    "\n",
    "Criei um exemplo bem simples para você poder ver passo a passo o que está acontecendo com os dados.\n",
    "\n",
    "Aqui eu já vou começar com um dicionário onde a chave seria o nome do arquivo e o valor o conteudo do arquivo em texto corrido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "political-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'get back': 'Jojo left his home in Tucson, Arizona\\nFor some California grass', 'something': 'Something in the way she moves\\nAttracts me like no other lover\\nSomething in the way she woos me', 'help': 'I need somebody\\nHelp! not just anybody\\nHelp! you know I need someone\\nHelp!'}\n"
     ]
    }
   ],
   "source": [
    "# Estou criando um dicionario com textos na mão\n",
    "docs = {\n",
    "    \"get back\": \"Jojo left his home in Tucson, Arizona\\nFor some California grass\",\n",
    "    \"something\": \"Something in the way she moves\\nAttracts me like no other lover\\nSomething in the way she woos me\",\n",
    "    \"help\": \"I need somebody\\nHelp! not just anybody\\nHelp! you know I need someone\\nHelp!\"\n",
    "}\n",
    "\n",
    "# Vamos imprimir o dicionario de documentos\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bound-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> get back\n",
      "Jojo left his home in Tucson, Arizona\n",
      "For some California grass\n",
      "-----------------\n",
      "-> something\n",
      "Something in the way she moves\n",
      "Attracts me like no other lover\n",
      "Something in the way she woos me\n",
      "-----------------\n",
      "-> help\n",
      "I need somebody\n",
      "Help! not just anybody\n",
      "Help! you know I need someone\n",
      "Help!\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "# Vamos imprimir mais um vez, so que mais bonito (vai facilitar para olharmos noss trabalho depois)\n",
    "for doc_name, doc_text in docs.items():\n",
    "    print('->', doc_name)\n",
    "    print(doc_text)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-shoulder",
   "metadata": {},
   "source": [
    "Agora o que queremos é processar os documentos como o exercício pede:\n",
    "\n",
    "> \"As palavras em cada uma das listas devem ser constituídas apenas por letras do alfabeto, estarem lexicamente normalizadas e conterem mais que 1 caracter.\"\n",
    "\n",
    "**Vou fazer primeiro fazendo um processamento por vez para vermos o que é feito em cada etapa. Depois eu junto tudo da maneira que fariamos na prática**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-publicity",
   "metadata": {},
   "source": [
    "#### 1) Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "starting-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tudo deve ser feito para todos os documentos, ou seja, devemos iterar pelos elementos do dicionario\n",
    "\n",
    "# Vamos salvar nesse dict as coisas\n",
    "docs_words = {}\n",
    "\n",
    "for doc_name, doc_text in docs.items():\n",
    "    words = nltk.word_tokenize(doc_text)\n",
    "    docs_words[doc_name] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fiscal-seattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> get back\n",
      "['Jojo', 'left', 'his', 'home', 'in', 'Tucson', ',', 'Arizona', 'For', 'some', 'California', 'grass']\n",
      "-----------------\n",
      "-> something\n",
      "['Something', 'in', 'the', 'way', 'she', 'moves', 'Attracts', 'me', 'like', 'no', 'other', 'lover', 'Something', 'in', 'the', 'way', 'she', 'woos', 'me']\n",
      "-----------------\n",
      "-> help\n",
      "['I', 'need', 'somebody', 'Help', '!', 'not', 'just', 'anybody', 'Help', '!', 'you', 'know', 'I', 'need', 'someone', 'Help', '!']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for doc_name, words in docs_words.items():\n",
    "    print('->', doc_name)\n",
    "    print(words)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-camel",
   "metadata": {},
   "source": [
    "#### 2) Transformar em minusculas e desconsiderar palavras com uma letra e com caracteres que não são letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "virtual-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, words in docs_words.items():\n",
    "    words = [w.lower() for w in words if len(w) > 1 and w.isalpha()]\n",
    "    docs_words[doc_name] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operating-spanish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> get back\n",
      "['jojo', 'left', 'his', 'home', 'in', 'tucson', 'arizona', 'for', 'some', 'california', 'grass']\n",
      "-----------------\n",
      "-> something\n",
      "['something', 'in', 'the', 'way', 'she', 'moves', 'attracts', 'me', 'like', 'no', 'other', 'lover', 'something', 'in', 'the', 'way', 'she', 'woos', 'me']\n",
      "-----------------\n",
      "-> help\n",
      "['need', 'somebody', 'help', 'not', 'just', 'anybody', 'help', 'you', 'know', 'need', 'someone', 'help']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for doc_name, words in docs_words.items():\n",
    "    print('->', doc_name)\n",
    "    print(words)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-cedar",
   "metadata": {},
   "source": [
    "#### 3) Normalização léxica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "for doc_name, words in docs_words.items():\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    docs_words[doc_name] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alive-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> get back\n",
      "['jojo', 'left', 'hi', 'home', 'in', 'tucson', 'arizona', 'for', 'some', 'california', 'grass']\n",
      "-----------------\n",
      "-> something\n",
      "['someth', 'in', 'the', 'way', 'she', 'move', 'attract', 'me', 'like', 'no', 'other', 'lover', 'someth', 'in', 'the', 'way', 'she', 'woo', 'me']\n",
      "-----------------\n",
      "-> help\n",
      "['need', 'somebodi', 'help', 'not', 'just', 'anybodi', 'help', 'you', 'know', 'need', 'someon', 'help']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for doc_name, words in docs_words.items():\n",
    "    print('->', doc_name)\n",
    "    print(words)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-trauma",
   "metadata": {},
   "source": [
    "#### Juntando tudo em um só laço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cardiac-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos salvar nesse dict as coisas\n",
    "docs_words = {}\n",
    "\n",
    "for doc_name, doc_text in docs.items():\n",
    "    words = nltk.word_tokenize(doc_text) # Separar em lista\n",
    "    words = [w.lower() for w in words if len(w) > 1 and w.isalpha()] # Filtrar palavras\n",
    "    words = [stemmer.stem(w) for w in words] # Normalização lexica\n",
    "    docs_words[doc_name] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crude-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> get back\n",
      "['jojo', 'left', 'hi', 'home', 'in', 'tucson', 'arizona', 'for', 'some', 'california', 'grass']\n",
      "-----------------\n",
      "-> something\n",
      "['someth', 'in', 'the', 'way', 'she', 'move', 'attract', 'me', 'like', 'no', 'other', 'lover', 'someth', 'in', 'the', 'way', 'she', 'woo', 'me']\n",
      "-----------------\n",
      "-> help\n",
      "['need', 'somebodi', 'help', 'not', 'just', 'anybodi', 'help', 'you', 'know', 'need', 'someon', 'help']\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "for doc_name, words in docs_words.items():\n",
    "    print('->', doc_name)\n",
    "    print(words)\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-toilet",
   "metadata": {},
   "source": [
    "#### Contando palavras repetidas\n",
    "\n",
    "Vamos usar o `docs_words` que acabamos de construir. Para contar repetições em uma lista basta transformar a lista em um conjunto (set) e de volta para lista, esse processo tira as repetições. Vamos processar um documento por vez, salvando o numero de repetições em um dicionário que iremos conferir no final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bright-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_rep = {}\n",
    "\n",
    "for doc_name, words in docs_words.items():\n",
    "    words_no_rep = list(set(words))\n",
    "    rep = len(words) - len(words_no_rep)\n",
    "    docs_rep[doc_name] = rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "explicit-viking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get back': 0, 'something': 6, 'help': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-recruitment",
   "metadata": {},
   "source": [
    "Agora só precimos pegar o elemento de um dicionário com a maior chave.\n",
    "\n",
    "Podemos usar o `sorted` do Python passando a lista de pares chave-valor que o `.items()` do dicionário retorna para transformar nosso dicionário em uma lista ordenada. Mas para isso precisamos dizer o que o Python deve considerar para as comparações, que no nosso caso é o valor (já que a chave é o nome do documento). Para isso usamos uma função lambda `lambda x: x[1]` isso significa o seguinte: para cada tupla x olhe para o elemento de indice 1 para fazer as comparações (numa tupla chave-valor o elemento de indice 1 é justamente o valor). Usamos `reverse=True` para ordenação decrescente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "french-meditation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('something', 6), ('help', 3), ('get back', 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(docs_rep.items(), key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
